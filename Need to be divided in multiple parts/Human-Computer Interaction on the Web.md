> [!Note]
> This document it's only available in Italian üáÆüáπ for now

> [!Note]
> Qua √® possibile trovare una lista di [[HCI - Domande e Risposte]] per l'esame orale.
## Wearable Devices

I wereable devices sono dei device che vengono indossati, il che li rende portabili, indossabili ed impiantabili nel corpo umano. Visto che l‚Äôuomo ed il device sono intrecciati l‚Äôun l‚Äôaltro, questo porta l‚Äôinterazione a richiedere molta poca attenzione. Molti wearable devices infatti non richiedono l‚Äôaccensione o lo spegnimento (constancy - constanza), e raccolgono dati ed effettuano azioni spesso in maniera automatica.

Sono device programmabili dall‚Äôutente, che utilizzano algoritmi avanzati e che raccolgono una grande quantit√† di dati.

L‚Äôuomo viene usato come feedback per la macchina, e la macchina come feedback all‚Äôuomo, quindi c‚Äô√® questo loop.

I wereable, come anche i dispositivi pi√π tradizionali (smartphones) possono interrompere l‚Äôattivit√† della persona. L‚Äôinterruzione viene classificata a seconda in varie categorie:

- **Immediate**: forzano l‚Äôutente a cambiare attivit√† ed a prestare attenzione. Un esempio sono la notifica di un allarme in casa.
- **Negoziate**: notificano l‚Äôutente senza distoglierlo dalla sua attivit√†, come ad esempio una notifica di un‚Äôapp di messagistica. L‚Äôutente pu√≤ guardare la notifica e reagire in futuro.
- **Programmate**: notificano l‚Äôutente periodicamente. Un esempio √® la sveglia.
- **Mediate**: sono notifiche che vengono inviate a seconda del contesto in cui l‚Äôutente si trova.

I primi tipi di wearables, risalenti dagli anni 1600, erano:

- Abaco su anello, il quale permetteva di effettuare semplici operazioni matematiche (1600);
- Orologio da polso (1800);
- Timing-device nella scarpa i modo da barare nel gioco della roulette (1960);
- Calcolatrice elettronica su orologio da polso (1970).

I wearables sono dotati di sensori ed attuatori. I primi acquisiscono dati relativi all‚Äôambiente, gli altri effettuano delle azioni a componenti meccaniche a seconda dei dati inviati dai sensori.

Questi dispositivi sono spesso dotati di comunicazione WiFi e Bluetooth.

L‚Äôinterazione avviene senza mouse e tastiera, ma spesso lo schermo √® touch, ed in pi√π l‚Äôutente pu√≤ effettuare delle gestures con il polso (in caso di smartwatch o smart bracelet), oppure coprire l‚Äôintero schermo con il palmo.

Il fattore chiave dell‚Äôinterazione √® anche il contesto in cui l‚Äôutente si trova. Quindi alcune azioni non potranno essere effettuate se l‚Äôutente si trova in una certa situazione, ad esempio agitare molto il polso in caso si √® in un meeting. Questo tipo di interazioni si chiamano *eyes-free*, visto che spesso non necessitano di guardare il dispositivo.

I casi d‚Äôuso di questo tipo di dispositivi sono:

- Monitoraggio dell‚Äôattivit√† fisica
- Monitoraggio del sonno
- Realt√† virtuale ed Aumentata
- Scopi medici

Alcuni contro di questi dispositivi sono:

- A lungo andare, possono essere scomodi da indossare;
- Possono causare troppe interruzioni, se queste non sono ben gestite dall‚Äôutente.

## Internet of Things

Per IoT si intende quella classe di oggetti che hanno al loro interno un computer, e che sono differenti da computer tradizionali o smartphones. Sono device connessi, non per forza ad Internet, ma connessi ad un qualche dispositivo che poi ha accesso ad internet.

Un esempio di tale dispositivi sono:

- Elettrodomestici Smart, i quali solitamente dispongono di un‚Äôapplicazione che permette di vedere delle statistiche, di controllarli o di programmare azioni anche a distanza. Questi solitamente si connettono direttamente ad Internet tramite il WiFi casalingho.
- Telepass: non √® connesso direttamente ad Internet, per√≤ si connette al dispositivo del casello per trasferire dei dati.
- Airtag: si connette ad ogni prodotto Apple nelle vicinanze, il quale √® poi connesso ad Internet, per aggiornare la sua posizione.

La UI su questo tipo di dispositivo solitamente √® limitata, a causa dello spazio disponibile e del costo.

Per quanto riguarda i modi in cui dare un input ad un dispositivo IoT:

- **Controlli fisici**: sono comodi perch√® sono tattili e possono essere quindi riconosciuti attraverso il tatto. La loro posizione √® fissa e quindi richiedono poca attenzione da parte dell‚Äôutente, se questo √® abituato ad usarli. Ovviamente non possono essere aggiornati e spostati in maniera semplice.
    - Bottoni
    - Manopole: possono essere con rotazione finita o infinita, e possono essere discreti o continui.
    - Switches: un bottone non rimane nello stesso stato in cui viene premuto, mentre uno switch si. Uno switch √® utile per mostrare lo stato del sistema, ma questo pu√≤ non essere accurato. Pensa a quando ci sono due switch che controllano la stessa luce.
- **Schermi Touch**: fanno in realt√† parte dei controlli fisici, ma tra i pro hanno il fatto che possono mostrare diverse interfacce a seconda dello stato, e la UI pu√≤ essere aggiornata tramite software. I contro sono che richiedeono grande attenzione dell‚Äôutente, visto che questo deve guardare lo schermo, ed il dito per cliccare su un bottone digitale.
- **UI Tangibili**: muovere oggetti in modo da dare un input al sistema. Un esempio √® un tipo di svegla che pu√≤ essere ruotata per cambiare le informazioni visualizzate sullo schermo. L‚Äôinterazione tangibile pu√≤ essere anche una forma di output, visto che la posizione dell‚Äôoggetto stesso descrive lo stato del sistema.
- **Riconoscimento vocale**: comodo perch√® hand-free ed eye-free, ma non pu√≤ essere usato in ogni contesto, visto che ci sono delle situazioni in cui non √® appropriato. Come pro ha anche che richiede poco spazio e poca capacit√† computazionale, visto che localmente solo l‚Äôaudio viene catturato, e la computazione viene effettuata sul Cloud.
- **Gestures:** permettono di interagire col sistema tramite movimenti del corpo.
- **Contesto**: il contesto in cui l‚Äôutente si trova in quel momento pu√≤ essere un input al sistema.
- **Pensieri**: ancora in fase di sviluppo, permettono di controllare il sistema tramite le onde cerebrali.
- **Battito cardiaco e fattori corporei**: permettono di determinare il sonno e altri fattori come la salute mentale.

Oltre agli input, i dispositivi IoT devono spesso dare un feedback all‚Äôutente. Le modalit√† di output sono le seguenti:

- **LED**: l‚Äôinformazione viene trasmessa attraverso il colore dei led, i pattern o il pattern del lampeggiamento del led. √à comodo perch√® poco costoso, anche in termini di energia utilizzata, e richiede molto poco spazio. Richiede poca attenzione da parte dell‚Äôutente, ma la quantit√† di informazione che si pu√≤ trasmettere √® molto limitata.
- **Schermi**: a seconda di quanta informazione deve essere trasmessa, si posso usare diverse tipologie di schemi. I 7-segment displays vengono usati nelle sveglie, visto che sono poco costosi e riesco a rappresentare ogni numero con un pattern di 7 segmenti. Negli e-book vengono usati gli schermi e-ink, che sono meno costosi in termini di energia, e non emettono luce, quindi non producono nessun danno agli occhi. Per le informazioni pi√π complesse bisogna far affidamento a schermi tradizionali, in bianco e nero o a colori. In generale prima di progettare un dispositivo bisogna chiedersi: ‚Äúquesta feature pu√≤ funzionare anche senza un display?‚Äù. (Altrimenti si incorre nel *feature creep*, in cui il prodotto viene riempito di funzionalit√†, la quale abbondanza rovina l‚Äôesperienza.)
- **Suoni e Text to Speech**: i suoni ed il parlato possono essere un altro mezzo di trasmissione delle informazioni. Come pro abbiamo il fatto che un testo parlato pu√≤ anche includere delle emozioni, cos√¨ come i suoni. Inoltre questo rende possibile un‚Äôinterazione con meno attenzione, visto che non bisogna ne toccare ne guardare il dispositivo. Come contro abbiamo il fatto che non sempre si √® in un contesto in cui √® appropriato emettere voci e suoni.
- **Vibrazioni, force feedback**: le vibrazioni possono essere utilizzate come feedback per far capire all‚Äôutente che una certa azione √® stata svolta.
- **Altri** (Odori e Temperatura): il sistema pu√≤ generare degli odori o cambiare temperatura per trasmettere delle informazioni.

Molti di questi dispositivi inoltre possiede delle batterie, quindi bisogna anche limitare il consumo dell‚Äôenergia al minimo. Una tecnica √® l‚Äôattivazione ad intermittenza, in modo da scambiare dati con una certa frequenza, limitando il periodo di accensione. Questo pu√≤ portare il sistema, soprattutto se fatto da pi√π interfacce su pi√π dispositivi, a non essere sincronizzato. Ad esempio se cambiamo la temperatura del condizionatore dall‚Äôapplicazione, e lo schermo del condizionatore di aggiorna dopo qualche minuto.

Quando una persona si connette ad internet, si aspetta di attendere un breve periodo di caricamento. Quando invece interagisce con oggetti reali, si aspetta che l‚Äôinterazione sia istantanea. Se stiamo parlando di un oggetto smart, e quindi connesso ad internet, l‚Äôinterazione potrebbe comunque richiedere del tempo, e quindi l‚Äôutente √® frustrato. 

Visto che i dispositivi sono connessi ad Internet, se questo smettesse di funzionare, si perderebbero molte funzionalit√†.

I dispositivi smart permettono di svolgere delle azioni su dispositivi fisici da remoto, o programmarli per essere eseguite automaticamente in un periodo futuro. Visto che questo aggiunge dell‚Äôastrazione (l‚Äôutente non interagisce fisicamente con l‚Äôoggetto) c‚Äô√® bisogno di un feedback che faccia capire che l‚Äôazione √® stata compiuta.

Visto che i dispositivi IoT sono prodotti da vari produttori differenti, c‚Äô√® bisogno che questi siano compatibili tra di loro il pi√π possibile, visto che un utente preferisce acquistare prodotti che si integrano bene tra di loro.

I dispositivi nell‚ÄôIoT si basano su una serie di principi, uno dei quali √® l‚Äôiterusabilit√†. Questo vuol dire che la UX si basa sull‚Äôusabilit√† di tutte le parti del sistema, e quindi bisogna progettare tutte le varie parti (dispositivo, applicazione mobile, applicaizone web, telecomando) in parallelo in modo da migliorare l‚Äôinterusabilit√†. L‚Äôinterusabilit√† viene calcolata come la quantit√† di QUALCOSA + efficienza e soddisfazione.

[Interusability: IoT User Experience Beyond The Level Of A Single Device | SpinDance](https://spindance.com/2018/09/07/interusability-iot-user-experience-beyond-level-single-device/)

![Screenshot 2023-06-27 at 4.47.03 PM.png](Screenshot_2023-06-27_at_4.47.03_PM.jpeg)

Questo √® lo stack di progettazione per dispositivi IoT, analizziamo le singole componenti:

- **UI Design**: si incentra sulla progettazione dell‚Äôinterfaccia, principalmente del layout, dell‚Äôestetica e delle sensazioni che questa provoca;
- **Interaction Design**: analizza l‚Äôintrazione che l‚Äôutente ha con il dispositivo.
- **Interusability**: vedi sopra.
- **Service Design**: si incentra sul capire qual √® il ciclo di vita dell‚Äôutente, analizzando gli aggiornamenti futuri e le nuove funzionalit√†, l‚Äôassistenza clienti, l‚Äôesperienza nello Store ecc.
- **Conceptual Model** (o Mental Model): √® il modello che l‚Äôutente ha del sistema, ovvero un qualcosa di pi√π astratto che permette all‚Äôutente di capire come il sistema funziona, evitando i dettagli tecnici.
- **Productisation**: analizza le funzionalit√†, i servizi e gli obiettivi che rendono speciale un tipo di prodotto, in modo da dare importanza al valore che il sistema aggiunge.
- **Platform Design**: si focalizza sul framework software che permette agli altri sviluppatori di utilizzare ed estendere il sistema, in modo da gestire gli utenti, le azioni ed i dati.

## Beacons

I beacons sono dei dispositivi che utilizzano la tecnologia Bluetooth Low Energy (BLE) introdotta nella versione 4.0 del protocollo Bluetooth. Questa tecnologia permette la comunicazione ad una distanza fino a 90 metri, ed opera nella banda di frequenza 2.4Ghz.

Con il Bluetooth 5.0 tale portata √® stata quadruplicata, la velocit√† raddoppiata nonch√® un auemnto di otto volte della capacit√† di trasmissione dei dati rispetto alla versione precedente.

[How Bluetooth Low Energy Works: Advertisements (Part 1)](https://novelbits.io/bluetooth-low-energy-advertisements-part-1/)

I beacons sfruttano la funzione *BLE advertising*, che consente a tali dispositivi, che operano come *broadcasters*, di trasmettere un ID e di mostrare dei servizi disponibili agli *observers*. Possono essere trasmessi anche altri dati, affinch√® questi siano solo pochi *bytes.* Un esempio di ulteriori dati possono essere la distanza dal beacon all‚Äô*observer*. **Ci√≤ permette agli *observer*, che solitamente sono smartphones, di ricevere il segnale quando sono vicini ad un certo beacon, e, a seconda dell‚ÄôID del beacon, effettuare determinate azioni.

I beacon fanno sia da *broadcasters (advertisers)*, che da *peripheral*. Un dispositivo *central* pu√≤ connettersi al beacon per leggere e scrivere dati. Solitamente questo dispositivo viene utilizzato per configurare o aggiornare il dispositivo beacon. 

In generale:

- Un *Broadcaster* √® un dispositivo che trasmette pacchetti tramite BLE per fornire informazioni riguardanti la presenza e l'identit√† ai dispositivi nelle vicinanze. Tale azione viene chiamata *advertising*, per questo ci si pu√≤ riferire a tale ruolo anche come *advertiser*.
- Un *peripheral* √® in generale un dispositivo che si connette ad un dispositivo *central* tramite BLE.
- L'*observer* √® il dispositivo che ascolta i pacchetti che vengono inviati dal *Broadcaster*, e che agisce di conseguenza.
- Il *Central* √® un dispositivo in grado di connettersi al *Broadcaster* e di leggere e scrivere i dati. Questo dispositivo viene solitamente utilizzato per configurare il beacon.

Visto che i beacon trasmettono una minima quantit√† di dati, la batteria ha una lunga durata, solitamente tra i 3 e i 5 anni.

Ci sono due differenti tipi di protocolli beacon:

- iBeacon, per Apple. Un dispositivo iOS pu√≤ ascoltare fino a 20 beacon simultaneamente. Quando il dispositivo riceve un segnale di un beacon vicino, il sistema operativo manda un segnale all‚Äôapplicazione interessata, la quale pu√≤ eseguire delle azioni.
- Eddystone, sviluppato da Google, √® supportato sia su Android che iOS.

Visto che il sistema operativo si occupa di gestire l'ascolto dei beacon nelle vicinanze, questo funzioner√† anche se lo schermo √® spento e l‚Äôapplicazione interessata √® chiusa.

Un applicazione pu√≤ sottoscriversi ad una regione, che √® un‚Äôarea in cui il segnale del beacon si propaga. Un azione pu√≤ essere svolta all‚Äôentrata o all‚Äôuscita di una certa regione. Questa funzionalit√† usa molta poca batteria dello smartphone.

Solamente quando l‚Äôapplicazione √® aperta ed in uso, √® possibile effettuare *ranging*, ovvero azionare dei comportamenti a seconda della prossimit√† del beacon. Questo pu√≤ essere svolto solamente in *foreground* dato l‚Äôalto consumo della batteria.

Il *ranging* √® utile per tracciare la posizione del dispositivo *indoor*, dove il segnale GPS non √® affidabile. Un ranging accurato viene effettuato tramite triangolazione, quindi vengono posizionati tre beacon all‚Äôinterno di un‚Äôarea, e l‚Äôincrocio delle tre circonferenze con raggio che va dal beacon al dispositivo permettono di localizzare, con un errore nell‚Äôordine dei metri, il dispositivo.

## UWB

L‚ÄôUltra Wide Band √® un tecnologia di trasmissione dati radio ad alta frequenza (pi√π di 500Mhz), che utilizza una bassa quantit√† di energia. Pu√≤ sostituire qualsiasi applicazione in cui sono presenti dei beacon, per√≤ il dispositivo √® pi√π costoso.

Attualmente viene usato per collezionara dati da sensori e per una precisa localizzazione, sopprattutto negli ambienti *indoor*. √à utilizzata negli AirTag.

Il supporto all‚ÄôUWB √® stato introdotto nei dispositivi di fascia alta a partire dal 2019.

L‚ÄôUWB permette di trasferire una grande quantit√† di dati, senza causare interferenze con altri segnali. Un dispositivo pu√≤ captare la presenza di un dispositivo UWB, e la sua posizione, con un errore nell‚Äôordine dei centimetri, misurando il ToF (Time of Flight) della trasmissione, che sarebbe il tempo che il segnale ci mette a raggiungere il beacon ed a tornare indietro.

## Chatbots

Un chatbot √® un programma che interagisce con l‚Äôutente attraverso una interfaccia a chat, in cui possono essere scambiati dei messaggi. Costruire un chatbot √® complicato in quanto devono essere usate conoscenze di NLP. 

I primi chatbot erano ‚Äúhard-coded‚Äù, e quindi l‚Äôutente aveva un limitato insieme di frasi da poter dire al chatbot, e questo rispondeva in maniera programmata.

Al giorno d‚Äôoggi, soprattutto dopo l‚Äôavvento dei Large Language Models (LLM) come GPT, i chatbot vengono usati in molte situazioni, ad esempio nell‚Äôassistenza in modo da ridurre il numero di operatori umani, e quindi ridurre i costi.

I chatbot vengono anche usati su applicazioni come Telegram e Discord, visto che un utente scarica tali applicazioni per parlare con delle persone, ma poi pu√≤ parlare anche con dei bot,  senza scaricare un‚Äôapplicazione apposta. In queste applicazioni spesso i bot sono un mix di chatbot e UI standard, visto che sono dotati di una serie di comandi che possono essere inseriti o cliccati. Nelle applicazioni di messaggistica, i chatbot vengono anche inseriti in gruppi per compiere delle azioni automatiche, come rilevare la spam e bloccare utenti malevoli, oppure effettuare sondaggi, leggendo i messaggi degli utenti nel gruppo.

Il problema con i chatbot pi√π antichi √® il fatto che la conversazione non mantiene uno stato, quindi non era possibile effettuare delle ‚Äúfollow up questions‚Äù o semplicemente continuare il discorso. Con l‚Äôavvento dei LLM questo √® un problema che ormai √® stato risolto, visto che tali modelli estremamente complessi hanno capacit√† di mantenere un contesto.

Quando avviene un errore, il chatbot dovrebbe rispondere in maniera umana, oltre che comunicare che tipo di errore si √® verificato ed eventualmente perch√®, cos√¨ l‚Äôutente pu√≤ capire e si sente meno frustrato.

In caso il chatbot ha un interazione a comandi pi√π ‚Äúhard-coded‚Äù, bisognerebbe dare dei consigli all‚Äôutente per fargli capire quali sono i comandi accettati.

Un chatbot pi√π avanzato, che passerebbe anche il Turing Test, dovrebbe far sapere all‚Äôutente che questo sta parlando con un bot e non con un umano.

## Voice User Interface

Una Voice User Interface √® l‚Äôestensione di un chatbot al campo eslusivamente vocale. Questa estensione ha delle limitazioni, come la lunghezza delle frasi, che nel chatbot pu√≤ essere molto pi√π estesa mentre in una VUI deve essere molto pi√π concentrata. 

Il dispositivo che integra una VUI, e che √® dotato di microfoni di input e altoparlanti per l‚Äôoutput (ed eventualmente anche di uno schermo) viene chiamato un Voice Command Device.

Le VUI hanno parecchi problemi che sono difficili da risolvere:

- Barge-In, ovvero la possibilit√† di interrompere l‚Äôassistente vocale mentre sta parlando, per aggiungere un qualcosa, per stopparlo o per cambiare argomento. In generale il capire quando √® il turno dell‚Äôutente e quando √® quello della VUI √® complicato e pu√≤ portare alla confusione;
- Identificare chi sta parlando tra una serie di persone;
- Come capire cosa √® possibile chiedere alla VUI, e cosa invece porterebbe ad un errore perch√® non √® stato implementato;
- Capire se il sistema ha effettivamente compreso la richiesta o no.
- Recall invece di Recognition: l‚Äôutente deve ricordare tutti i possibili comandi, invece di selezionarli da una lista.
- L‚Äôattesa tra la richiesta e la risposta pu√≤ portare la persona a pensare che la VUI non abbia capito, e quindi alla confusione ed al ripetere la richiesta, il che causerebbe degli errori nella VUI.

In generale, le Voice User Interfaces fanno sentire l‚Äôutente meno in controllo rispetto alle UI tradizionali, per questo dovrebbero essere molto minimali, in modo da poter eseguire una funzionalit√† nella maniera pi√π semplice possibile, senza far confondere l‚Äôutente.

L‚Äôinterazione con le VUI possono essere di tre tipi:

- Command and Control: interazione *one-way*, l‚Äôutente utilizza la voce per dare comandi al sistema. L‚Äôoutput √® tradizionale.
- L‚Äôinterazione *one-way* opposta, in cui l‚Äôutente trasmette l‚Äôinput in maniera tradizionale, e l‚Äôoutput √® sottoforma di voce.
- Spoken Dialogue System: interazione *two-way* in cui avviene un dialogo tra l‚Äôuomo e la macchina, tutti e due tramite la voce.

Una tradizionale pipeline per la gestione di un dialogo tra uomo e una VUI consiste nelle seguenti fasi:

1. Riconoscere che l‚Äôutente sta interpellando l‚Äôassistente vocale;
2. Capire chi, tra una serie di utenti, sta facendo la richiesta;
3. Riconoscere cosa l‚Äôutente sta dicendo attraverso un sistema di Automatic Speech Recognition;
4. Interpretare la richiesta dell‚Äôutente tramite un sistema di Natural Language Understanding;
5. Seleziona l‚Äôazione ottimale a seconda dell‚Äôintento dell‚Äôutente, considerando eventualmente anche il contesto attuale;
6. Generare una risposta in linguaggio naturale;
7. Trasformare la risposta da testo al parlato tramite un sistema di Text to Speech.

Progettare una VUI richiede molteplici competenze, come informatiche, linguistiche e psicologiche.

Cos√¨ come in tutti gli altri sistemi, anche nelle VUI ci sono gli utenti esperti, i quali hanno pi√π dimestichezza con il sistema. Per questo motivo bisogna inserire delle scorciatoie in modo che questi possano effettuare delle azioni in maniera pi√π efficiente. Un esempio √® quella di inserire tutti i parametri in una sola frase invece che essere guidati da una serie di domande e risposte.

## Haptic Interaction

L‚Äôinterazione tattile (haptic) non √® solo un modo per dare input al sistema, ma anche per ricevere un feedback. √à possibile ad esempio dare l‚Äôinput alla macchina toccando degli oggetti virtuali, i quali faranno scattare un trigger tattile che far√† capire all‚Äôutente che ha effettivamente toccato quell‚Äôoggetto.

L‚Äôinterazione tattile √® utile quando non si pu√≤ utilizzare il feedback visivo, magari perch√® in situazioni particolari, come la guida, o magari per condizioni di cecit√†. Tramite l‚Äôinterazione tattile, l‚Äôutente pu√≤ percepire la forma di un tasto virtuale, o capire l‚Äôazione compiuta solamente dal feedback tattile. Per esempio Apple Watch permette di capire l‚Äôora solo tramite le vibrazioni, se ci si trova in una situazione in cui non √® adeguato guardare l‚Äôorologio.

L‚Äôinterazione tattile si basa su differenti capacit√† umane:

- **Percezione tattile**: l‚Äôabilit√† di percepire cose a contatto con la pelle.
- **Senso cinestetico** (Kinesthetic Sense): l‚Äôabilit√† di capire dove solo le parti del corpo senza effettivamente guardarle.
- **Percezione cinestetica** (Kinesthetic Perception): dipende dal senso cinestetico, ed √® l‚Äôabilit√† di capire dove sono le parti del corpo anche senza il senso tattile (si pensi quando vengono usati degli anestetici).
- **Percezione tattile passiva** (Passive Haptic Perception): la capacit√† di capire il posizionalemnto del corpo dalle informazioni tattili. Passivo perch√® le informazioni vengono dal mondo esterno e il cervello le elabora soltanto.
- **Copia dell‚ÄôEfferenza** (Efference Copy): consiste nella predizione del movimento e delle azioni da questo causate. Questo √® utile perch√® l‚Äôeffettivo movimento pu√≤ essere aggiustato a seconda della predizione.

La percezione visiva √® pi√π veloce di quella tattile, ma in alcuni casi la percezione tattile pu√≤ essere pi√π affidabile, ad esempio per capire la texture di un materiale. Ovviamente per altri casi la percezione visiva pu√≤ essere pi√π affidabile.

In generale, i simboli rialzati sono pi√π facili da distinguere dei simboli incisi.

Nella storia, sono stati fatti dei tentativi per dei display tattili, che utilizzavano le vibrazioni e delle parti scorrevoli per simulare le texture. In generale ci sono vari modi per ottenere un feedback tattile, dalla vibrazione, all‚Äôabrasione della pelle, all‚Äôuso di corrente elettrica. 

Ci sono varie limitazione nell‚Äôinterazione tattile, tra cui:

- Risoluzione spaziale: la distanza minima per cui due punti vengono percepiti come uno solo.
- Risoluzione temporale: il tempo affinch√® la persona percepisca l‚Äôoggetto tramite il tatto, solitamente tra 2 a 40ms.
- Interazione tra stimoli ampiamente distanziati: si potrebbe verificare l‚Äôevento del ‚Äúpunto fantasma‚Äù quando due punti vengono presentati a tutte e due le mani.
- Attenzione limitata: l‚Äôincapacit√† di concentrarsi quando c‚Äô√® un ecesso di informazioni.

In un dispositivo, il Degree of Freedom (DOF) √® il numero di assi in cui questo pu√≤ essere mosso. Un volante ad esempio ha un solo DOF, visto che pu√≤ essere mosso solo a destra e sinistra, mentre una penna ha 2 DOF, in caso di pressione anche 3 DOF.

Esistono anche dispositivi con 6 DOF (pi√π dei 3 assi spaziali), visto che questi possono essere mossi nel mondo tridimensionale, ma si pu√≤ arrivare in un punto mediante strade diffrerenti. Questo √® possibile perch√® ci sono due parti che entrambe hanno 3 DOF (two rotating arms). In generale, pi√π DOF significa pi√π facilit√† nel muovere l‚Äôoggetto.

Per Haptic Rendering si intende l‚Äôeffettuare una forza opposta quando l‚Äôoggetto virtuale viene toccato, in maniera da dare l‚Äôillusione della solidit√†.

Questo tipo di oggetti dovrebbe apparire invisibile all‚Äôutente, in modo da dare l‚Äôillusione che l‚Äôoggetto che effettua il feedback tattile (come dei guanti nella realt√† virutale) non sia presente affatto.

Il loop tattile ha una frequenza maggiore del loop visivo (1000Hz contro i 60Hz), visto che la tattilit√† ha una risoluzione temporale maggiore.

## Tangible Interaction

Per interazione tangibile si intende quel tipo di interazione in cui √® possibile dare un input al sistema muovendo degli oggetti reali.

Esempi di interazione tangibile sono:

- Prendere in mano il telefono da una superfice per attivare lo schermo;
- Bussare sullo schermo per fare uno screenshot;
- Mettere il telefono a faccia in gi√π su una superficie per attivare la modalit√† ‚Äúnon disturbare‚Äù.

Un altro esempio pi√π sperimentale √® la *Marble Answering Machine*, ovvero un prototipo di segreteria telefonica tangibile, in cui i messaggi erano modellati da delle biglie, e che quindi potevano essere sentire semplicemente ponendo la biglia relativa sulla macchina.

Un altro esempio di TUI sono le *Graspable Interfaces*, le quali permettono di interagire con oggetti fisici che vengono posti sopra lo schermo, in modo da interagire con le forme digitali che vengono visualizzate sullo schermo. 

Il pro principale delle TUI √® la loro intuitivit√†, in quanto basta manipolare oggetti fisici per interagire con il sistema. Il contro √® che questa interazione richiede spazio e tecnologie particolari.

Si definisce come *affordance* le propriet√† di un oggetto che suggerisce come questo pu√≤ essere usato. Ad esempio un pomello suggerisce che questo pu√≤ essere girato, un bottone di essere premuto ecc.

Gli oggetti tangibili possiedono un‚Äô*affordance* reale, quelli virtuali un‚Äô*affordance* percepita.

√à possibile combinare la realt√† aumentata ad una interfaccia tangibile. Un esempio √® un libro che mostra in realt√† aumentata gli animali che vengono mostrati sulla pagina. Quando si gira pagina (interazione tangibile), cambia anche il relativo animale mostrato.

Le TUI utilizzano il modell MCR (*Model-Control-Representation*), in cui gli oggetti tangibili rappresentano lo stato del sistema, quindi anche se il sistema viene spento, l‚Äôutente √® ancora in grado di percepire lo stato del sistema (totale o parziale). Il modello √® rappresentato dai dati digitali, mentre il controllo avviene con gli oggetti reali.

Possiamo classificare le TUI in tre categorie:

- Superfici Interattive, come *tangible bits.*
- Assemblamento Costruttivo, come *sandscape.*
- Token + Vincoli, come la *Marble Answering Machine*

Il contro principale delle TUI √® il fatto che una funzione di *undo* non pu√≤ essere implementata, e che lo stato precedente ad un‚Äôazione va ricordato, altrimenti questo viene perso. Inoltre non esiste il concetto di macro per facilitare una serie di azioni svolta frequentemente.

Ogni TUI viene progettata per un singolo scopo, e difficilmente pu√≤ essere espansa a pi√π scopi. Possono inoltre causare fatica con un utilizzo prolungato.

Le tecnologie principali per lo sviluppo di TUI sono:

- RFID, per l‚Äôidentificazione di oggetti sulla superfice. Gli RFID sono comodi perch√® non richiedono una batteria.
- Computer Vision, permettono non solo di capire se un oggetto √® presente o meno, o di identificare tale oggetto, ma anche di capire la sua forma, il colore, la posizione esatta, la forma e l‚Äôorientamento.
- Microcontrollori, sensori ed attuatori.

## Gestural Interaction

Per interazione *gestuale* si intende quel tipo di interazione con il sistema attraverso movimenti con il corpo. Esistono due tipi di *gestures*:

- Touch Gestures: effettuate su una superfice touch, permettono di interagire in maniera pi√π intuitiva con il sistema. Un esempio di queste interazioni sono lo *swipe* o il *pinch*.
- Touchless Gestures: vengono effettuate nello spazio tridimensionale, senza toccare nessun dispositivo. Vengono principalmente usate in VR e AR.
Nella restante parte, ci focalizzeremo su questo tipo di Gestures.

Le gestures possono essere catturate attraverso diversi sensori, come accelerometri, giroscopi (ad esempio per il *wake up* di uno smartwatch quando si gira il polso). camere con sistemi di *computer vision* (come il *Kinect*), camere ad infrarossi, scanner laser e camere stereoscopiche.

I gesti possono essere catturati con o senza la consapevolezza dell‚Äôutente.

- Se l‚Äôutente non √® consapevole, i gesti saranno naturali;
- Se l‚Äôutente √® consapevole, i gesti sono influenzati dal fatto che cercher√† di renderli pi√π comprensibili dal sistema.

A volte interagire con un sistema attraverso dei gesti potrebbe non essere socialmente accettabile, questo dipende soprattutto in che contesto ci si trova.

Un altro fattore da considerare √® l‚Äôeffetto *Gorilla Arm*, in cui l‚Äôinterazione porta ad un affaticamento nel lungo periodo.

Per quanto riguarda la realt√† virtuale, questa presenta vari problemi. Uno di dato dal fatto che lo spazio virtuale √® molto pi√π grande dello spazio reale in cui la persone √® immersa, quindi non si pu√≤ liberamente camminare nello spazio reale, ma bisogna teletrasportarsi utilizzando appositi comandi nello spazio virtuale, il che rende l‚Äôesperienza meno realistica.

Definiamo come:

- Fedelt√† dell‚Äôinterazione (Interaction Fidelity): il grado con cui le azioni all‚Äôinterno della UI corrispondono a quelle effettuate nel mondo reale. Ad esempio quanto devo muovere il braccio per effettuare uno spostamento nel mondo virtuale. Maggiore la fedelt√†, pi√π realistica l‚Äôinterazione.
- Movimenti ipernaturali (Hypernatural Movements): la possibilit√† di fare movimenti nel mondo virtuale che non sarebbero possibili nel mondo reale. Questo non intacca la fedelt√† visto che l‚Äôutente √® consapevole di star facendo movimenti possibili sono nel mondo virtuale.

Visto che solitamente l‚Äôinterazione gestuale non √® molto affidabile, si possono usare altri dispositivi di input in aiuto, come i joystick nel caso della maggior parte dei visori di realt√† virtuale.

Un modo per aumentare l‚Äôaffidabilit√† dei gesti √® quello di prendere la media nell‚Äôintorno di vari punti quando l‚Äôutente sta cercando di rimanere fermo, in modo da evitare che il cursore oscilli. Si pu√≤ anche modificare il raggio control/display, ovvero l‚Äôutente deve fare un movimento ampio per muovere di poco il cursore nel mondo virtuale, in modo che questo sia pi√π preciso.

Quando si ha a che fare con le gestures, non √® possibile lasciare il cursore da qualche parte come viene effettuato con il mouse, che pu√≤ rimanere ‚Äúparcheggiato‚Äù.

## Zooming Interfaces

Quando si naviga in interfacce classiche, come quelle di una webapp, si fa fatica a capire con quale sequenza di azioni ci si √® trovati in una certa pagina. √à come se fosse un labirinto.

Una zooming UI √® un‚Äôinterfaccia che si presenta come una tela infinita, con risoluzione infinita, in cui l‚Äôutente pu√≤ fare ‚Äúzoom out‚Äù per vedere la situazione generale, e poi fare ‚Äúzoom in‚Äù per vedere sempre pi√π dettagli.

Un esempio di sistemi che utilizzano questo tipo di interfacce sono Figma, Excalidraw e Google Maps.

Lo zoom imposta una gerarchia, in maniera tale che il sistema possa effettuare delle azioni a seconda del livello di zoom in cui l‚Äôinterfaccia si trova in quel momento. Ad esempio in google maps quando lo zoom √® ridotto vengono caricate delle texture in bassa risoluzione, la quale aumenta quando si effettua uno zoom maggiore.

Il pro delle ZUI √® che l‚Äôutente pu√≤ organizzare il contenuto della tela nella maniera preferita, e pu√≤ vedere il quadro generale, quindi diminuendo la probabilit√† di perdersi all‚Äôinterno del contenuto come pu√≤ avvenire per sistemi complessi con UI tradizionali, visto che qui l‚Äôutente si ricorda la posizione attraverso *landmarks*, ovvero punti di riferimento, invece che attraverso una sequenza di pagine.

## HCI nell‚Äôautomobile

Nel corso degli ultimi anni le automobili sono diventate sempre pi√π smart e connesse. Possiedono CPU, sensori ed attuatori sempre pi√π potenti ed affidabili.

Nelle auto sono presenti diversi sistemi:

- Navigazione ed Intrattenimento;
- Interfacce per il cellulare, in maniera da utilizzare alcune funzionalit√† del telefono in maniera sicura, senza prestare troppa attenzione;
- HVAC (Heating, Ventilation and Air Conditioning).

La connessione del telefono all‚Äôauto porta a dei problemi, ad esempio se nell‚Äôauto ci sono pi√π persone, tutte con il bluetooth attivo, quale smartphone dovrebbe connettersi all‚Äôauto?

In generale, l‚ÄôHCI all‚Äôinterno dell‚Äôautomobile cerca di ridurre le distrazioni del guidatore. Queste distrazioni possono essere di due tipi:

- Visuali: la persona sta guardando qualche altra cosa;
- Cognitive: la persona sta pensando a qualche altra cosa, e quindi non sarebbe in grado di reagire velocemente ad un evento improvviso.

Ci sono sistemi di sicurezza che consentono all‚Äôauto di rilevare un ostacolo sulla strada e di frenare di conseguenza, ma il punto √® capire perch√® il guidatore era distratto.

Nel contesto della guida, gli schermi touch sono un rischio alla sicurezza. I pulsanti fisici possono distrarre meno, visto che hanno una natura tattile, e la loro posizione non cambia. Per i comandi touch la persona deve vedere dov‚Äô√® il pulsante, vedere dov‚Äô√® il dito, cliccare e poi vedere se il contesto √® effettivamente cambiato, per capire se l‚Äôazione √® stata effettivamente presa.

Molte auto hanno dei bottoni fisici sul volante, visto che √® quello il posto dove naturalmente risiedono le mani. Questi sono comodi e non distraggono il guidatore. Quando si gira il volante sono difficili da raggiungere, oltre che a capire la loro posizione, ma spesso si pu√≤ aspettare che il volante sia di nuovo nella posizione corretta per premere un bottone.

In caso di azioni critiche, come fermare il *cruise control* perch√® c‚Äô√® del traffico improvviso davanti, deve esserci un controllo raggiungibile istantaneamente. Nel caso del *cruise control*, questo pu√≤ essere disattivato premendo il pedale del freno.

A volte anche gli input possono distrarre, ad esempio controllare il volume dello stereo. Esistono alcuni sistemi che puntano a ridurre le distrazioni, come l‚Äô*head up display*, che permette di visualizzare alcune informazioni sul cruscotto cos√¨ da non distogliere lo sguardo dalla strada.

Un modo per mitigare le distrazioni √® quello di usare interazioni *hand-free,* come la voce per l‚Äôinput e i suoni o la voce per l‚Äôoutput. Il problema √® che l‚Äôinput vocale non √® affidabile, e a volte non appropriato (ad esempio se qualcuno sta dormendo). 

Le interfacce gesturali a volte possono essere pi√π comode ed affidabili. Il problema principale con queste √® che possono essere ambigue, e comunque serve controllare lo stato del sistema per ricevere un feedback sull‚Äôavvenimento o meno dell‚Äôazione, per capire se la giusta azione √® stata compiuta.

Nell‚Äôauto esistono un sacco di sistemi di cui l‚Äôutente non ha il controllo, e di cui potrebbe anche esserne completamente ignaro, come l‚ÄôABS. Per altri invece si vuole avere il controllo, come il parcheggio automatic, a volte il guidatore vuole parcheggiare, a volte magari vuole che l‚Äôauto lo faccia per lui.

## Context Aware Interaction

Capire il contesto in cui l‚Äôutente si trova √® essenziale per migliorare l‚Äôusabilit√† del sistema. A seconda del contesto, l‚Äôinterfaccia potrebbe cambiare automaticamente, o eseguire altre azioni, anche senza l‚Äôattenzione dell‚Äôutente.

Qualche esempio di sistemi *context-aware* sono:

- Porte automatiche;
- Cambio automatico tra modalit√† chiara e scura a seconda dell‚Äôora del giorno o della quantit√† di luce nell‚Äôambiente circostante;
- Modalit√† ‚Äúauto‚Äù automatica nelle applicazioni di riproduzione musicale quando si √® connessi ad un automobile;
- Riposta automatica ai messaggi ed alle chiamate quando si sta guidando.
- Sistema di ABS nelle auto. Questo monitora costantemente il bloccaggio delle ruote e, quando questo avviene, rilascia a intermittenza i freni in modo da evitare che le ruote si blocchino.

Per rendere tutto questo possibile, √® necessario un algoritmo (spesso implementato tramite ML) che permetta di classificare il contesto a seconda dei dati catturati da uno o pi√π sensori.

Nel caso degli smarthone, tale algoritmo √® implementato nel sistema operativo, e permette di catturare dei contesti comuni, ad esempio quando l‚Äôutente sta camminando, correndo, dormendo ecc. Tale contesto pu√≤ essere poi usato dall‚Äôapplicazione interessata per effettuare le azioni necessarie.

Nel 1991 qualcuno diceva che la tecnologia dovrebbe essere trasparente all‚Äôumano, ed i sistemi *context-aware* permettono proprio questo, visto che agiscono in maniera completamente trasparente all‚Äôutente a seconda del contesto in cui questo si trova.

A volte il contesto percepito dal sistema √® differente dal contesto percepito dalla persona. Ad esempio per quanto riguarda le luci ad accensione automatica con un sensore di movimento, per il sistema il contesto √® ‚Äúsi √® verificato un movimento‚Äù, mentre per la persona √® ‚Äúsono all‚Äôinterno della stanza‚Äù. Questo causa una peggiore esperienza utente, visto che in mancanza del movimento la luce verr√† spenta, anche se la persona si trova ancora all‚Äôinterno della stanza.

Il contesto umano √® basato su ricordi ed esperienza, mentre quello percepito dal sistema √® basato su sensori ed algoritmi. Questo fenomeno si chiama *Awareness mismatch*.

A volte un sistema dovrebbe¬†poter dare dei consigli all‚Äôutente circa qual √® il contesto rilevato, in maniera tale che l‚Äôutente possa capire cosa c‚Äô√® che non va quando c‚Äô√® un *mismatch*.

Il contesto pu√≤ essere utilizzato anche per creare dei contenuti automatici, ad esempio mandare un messaggio ad un contatto SOS quando viene rilevata una caduta o un incidente d‚Äôauto.

Il contesto in generale √® molto utile per anticipare ci√≤ che l‚Äôutente vuole, il che √® un‚Äôoperazione difficile.